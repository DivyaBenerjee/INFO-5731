{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "l1=[i*10 for i in range(1,11)]\n",
        "l2=[]\n",
        "for i in l1:\n",
        "  URL = 'https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start={}'.format(i)\n",
        "  page = requests.get(URL)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  for i in soup.find_all(\"div\",{\"class\":\"snippet\"}):\n",
        "    l2.append(i.text)\n",
        "import pandas as pd\n",
        "df=pd.DataFrame({\"Abstract\":l2})\n",
        "df.to_csv(\"my1csv.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "JpesrDl68DhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "j3-J3lxbWtXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "vk7ZU5EGWw4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "df = pd.read_csv(\"my1csv.csv\")\n",
        "\n",
        "#Removing special characters\n",
        "df[\"Abstract_cln\"] = df[\"Abstract\"].str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\n",
        "\n",
        "#Punctuation removal\n",
        "df['Abstract_cln'] = df['Abstract_cln'].str.replace('[^\\w\\s]','')\n",
        "\n",
        "#remove numbers\n",
        "df['Abstract_cln'] = df['Abstract_cln'].str.rstrip(string.digits)\n",
        "\n",
        "#Stopwords removal\n",
        "stop = stopwords.words('english')\n",
        "df['Abstract_cln'] = df['Abstract_cln'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "#Lower casing\n",
        "df['Abstract_cln'] = df['Abstract_cln'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "\n",
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df['Abstract_cln'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "\n",
        "#Lemmatization\n",
        "df['Abstract_cln'] = df['Abstract_cln'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ],
      "metadata": {
        "id": "UPG16HddW26Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "jvxgJKAAW52z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag=[]\n",
        "frequency=[]\n",
        "j=0\n",
        "for i in df['Abstract_cln']:\n",
        "  token=nltk.word_tokenize(i)\n",
        "  pos_tag.append(nltk.pos_tag(token,tagset='universal'))\n",
        "  frequency.append(nltk.FreqDist(tag for (word, tag) in pos_tag[0]))\n",
        "  print(pos_tag[j])\n",
        "  print(frequency[j])\n",
        "  j=j+1\n",
        "print(frequency)"
      ],
      "metadata": {
        "id": "LBMHyZ5-W-yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install benepar"
      ],
      "metadata": {
        "id": "H8ALzcYlXGbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x\n",
        "import spacy\n",
        "import benepar\n",
        "from benepar.spacy_plugin import BeneparComponent\n",
        "benepar.download('benepar_en')"
      ],
      "metadata": {
        "id": "H3hr4tFSXMPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "from spacy import displacy\n",
        "for i in df['Abstract_cln']:\n",
        "  displacy.render(nlp(i),jupyter=True)"
      ],
      "metadata": {
        "id": "cLhkwpImXQAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "ner_dict=[]\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "for i in df['Abstract_cln']:\n",
        "  for entity in nlp(i).ents:\n",
        "    ner_dict.append((entity.label_,entity.text))\n",
        "    print(entity.label_,entity.text)"
      ],
      "metadata": {
        "id": "aAGXLhwhXTqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=df.head(100)\n",
        "data"
      ],
      "metadata": {
        "id": "ZPW-2qxNXfu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('cleaned_data.csv')"
      ],
      "metadata": {
        "id": "5p_t9YzfXi-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Aft41j3-Xm4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "my_data = open('cleaned_data.csv', \"r\")\n",
        "unigrams=ngrams(my_data.read().split(),1)\n",
        "print(unigrams)\n",
        "unigra_freq_dist=nltk.FreqDist(unigrams)"
      ],
      "metadata": {
        "id": "8O6g2cKfXplb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('cleaned_data.csv')\n",
        "raw = f.read()\n",
        "\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "\n",
        "#Create your bigrams\n",
        "bgs = nltk.bigrams(tokens)\n",
        "\n",
        "#compute frequency distribution for all the bigrams in the text\n",
        "fdist = nltk.FreqDist(bgs)\n",
        "for k,v in fdist.items():\n",
        "    print(k,v)"
      ],
      "metadata": {
        "id": "f28JTQ7cXsE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('cleaned_data.csv')\n",
        "raw = f.read()\n",
        "\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "\n",
        "#Create your bigrams\n",
        "bgs = nltk.trigrams(tokens)\n",
        "\n",
        "#compute frequency distribution for all the bigrams in the text\n",
        "fdist = nltk.FreqDist(bgs)\n",
        "for k,v in fdist.items():\n",
        "    print(k,v)"
      ],
      "metadata": {
        "id": "woC2Hbh3X6eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating bigram for future use in the assignment\n",
        "clean_data = open('cleaned_data.csv', \"r\")\n",
        "bigrams = ngrams(clean_data.read().split(), 2)\n",
        "bigra_freq_dist=nltk.FreqDist(bigrams)"
      ],
      "metadata": {
        "id": "IrVfqcTnYhSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a fucntion which will give probability of  all the bigrams in the dataset using the given formula\n",
        "def probability_bigram():\n",
        "  for bigram_values in bigra_freq_dist:\n",
        "#    print(bigram_values)\n",
        "    print((\" The bigram values\", bigram_values,\"Probability :\",bigra_freq_dist[bigram_values]/unigra_freq_dist[(bigram_values[0],)]))\n",
        "\n",
        "probability_bigram()"
      ],
      "metadata": {
        "id": "LZt42VXtYkMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from textblob import TextBlob\n",
        "clean_data = open('cleaned_data.csv', \"r\")\n",
        "\n",
        "_noun = lambda pos: pos[:2] == 'NN'\n",
        "tokenized = nltk.word_tokenize(clean_data.read())\n",
        "nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if _noun(pos)] \n",
        "\n",
        "print(\"The Nouns:\\n\",nouns)"
      ],
      "metadata": {
        "id": "e754fsQ3Ytnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "clean_data = open('cleaned_data.csv', \"r\")\n",
        "\n",
        "blob = TextBlob(clean_data.read())\n",
        "noun_phrases = blob.noun_phrases\n",
        "print(noun_phrases) "
      ],
      "metadata": {
        "id": "o7Kt139fZBGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "fPIn3rh3ZHuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_noun = pd.DataFrame(noun_phrases, columns = ['noun_phrases'])\n",
        "df_noun "
      ],
      "metadata": {
        "id": "Hv-uZX9UZKx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data = open('cleaned_data.csv', \"r\")\n",
        "f = nltk.FreqDist(noun_phrases)\n",
        "f "
      ],
      "metadata": {
        "id": "gYGyY-egZN0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(20 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "my_tf_result = (df['Abstract_cln']).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
        "my_tf_result.columns = ['Abstract_words','my_tf']\n",
        "my_tf_result\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2.2\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import numpy.linalg as LA"
      ],
      "metadata": {
        "id": "OhKkJY2tZYh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tn_set = df['Abstract_cln'].values.tolist() #train set\n",
        "my_tt_set = \"introduction statistical natural language processing snlp field lying intersection natural language processing machine learning snlp di er traditional natural language processing \" #Query taken from one of the tweets which is called test set\n",
        "my_tt_set = [my_tt_set]\n",
        "stopWords = stopwords.words('english')\n",
        "vzer = CountVectorizer(stop_words = stopWords)\n",
        "\n",
        "transformer = TfidfTransformer()"
      ],
      "metadata": {
        "id": "3AaDPQ3pZbJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tnVectorizerArray = vzer.fit_transform(tn_set).toarray()\n",
        "ttVectorizerArray = vzer.transform(my_tt_set).toarray()\n",
        "cx = lambda a, b : np.inner(a, b)/(LA.norm(a)*LA.norm(b))\n",
        "cosine_values = []"
      ],
      "metadata": {
        "id": "N_k-YFvyZd28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for vector in tnVectorizerArray:\n",
        "        for testV in ttVectorizerArray:\n",
        "            cosine = cx(vector, testV)\n",
        "            cosine_values.append(cosine)"
      ],
      "metadata": {
        "id": "bv-SftWNZgk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = df.filter(['Abstract_cln'], axis=1)\n",
        "se = pd.Series(cosine_values)\n",
        "df_result['Cosine_similarity'] = se.values\n",
        "df_result"
      ],
      "metadata": {
        "id": "IpBfYcR9Zjqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3idDo9UeV9jY"
      },
      "source": [
        "# **Question 3: Create your own word embedding model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_auu7TXqA1hK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unaR-nZTV9jZ"
      },
      "source": [
        "(20 points). Use the data you collected for assignment two to build a word embedding model: \n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim "
      ],
      "metadata": {
        "id": "y89LKE_4geVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")\n",
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "fmJUmqG7iRR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = 'https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start={}'\n",
        "model = Word2Vec(sentences)"
      ],
      "metadata": {
        "id": "c4e-KLfWjxv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.save_word2vec_format('model.bin')\n",
        "model.wv.save_word2vec_format('model.txt', binary=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b4k0TveflwLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "\t\t\t['yet', 'another', 'sentence'],\n",
        "\t\t\t['one', 'more', 'sentence'],\n",
        "\t\t\t['and', 'the', 'final', 'sentence']]"
      ],
      "metadata": {
        "id": "fSMo0jqB7dcm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)"
      ],
      "metadata": {
        "id": "6tALWCFJ7mJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the loaded model\n",
        "print(model)\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "ia0nLOe37qjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# access vector for one word\n",
        "print(model['sentence'])\n",
        "# save model\n",
        "model.save('model.bin')\n",
        "# load model"
      ],
      "metadata": {
        "id": "JUuoKvPm7rg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = Word2Vec.load('model.bin')\n",
        "print(new_model)"
      ],
      "metadata": {
        "id": "P-o5Jp9y7v7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "\t\t\t['yet', 'another', 'sentence'],\n",
        "\t\t\t['one', 'more', 'sentence'],\n",
        "\t\t\t['and', 'the', 'final', 'sentence']]"
      ],
      "metadata": {
        "id": "Xy6fJPCQ7zQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# fit a 2d PCA model to the vectors\n",
        "X = model[model.wv.vocab]"
      ],
      "metadata": {
        "id": "L5P8lN3o74xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "WiAe7sPX77tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 4: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S3OzckG57czH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AjsG29Hzga_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "outputs": [],
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "\n",
        "\n",
        "# Link: https://colab.research.google.com/drive/16mgdhaARF5GVV4kTJClIn1mzg8SfkFSb#scrollTo=XfvMKJjIXS5G&uniqifier=1\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "INFO5731_Assignment_Three-1.ipynb_divya",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}